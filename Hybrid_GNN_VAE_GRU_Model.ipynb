{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kaynmBtgxga7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b82bd9d-f950-4742-b5e3-81a540d9a0fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imporing the neccecery lib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch_geometric.nn import GATConv, GCNConv, GraphConv\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import time\n"
      ],
      "metadata": {
        "id": "20x-CUyRxiWU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed setting for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()\n"
      ],
      "metadata": {
        "id": "yMSJJOf3x0k6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention module for feature enhancement\n",
        "class FeatureAttention(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(FeatureAttention, self).__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(in_features, in_features // 2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(in_features // 2, in_features),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        att_weights = self.attention(x)\n",
        "        return x * att_weights\n"
      ],
      "metadata": {
        "id": "TrQBSCA7x5MQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hybrid GNN-VAE-GRU Model\n",
        "class EnhancedHybridGNNVAEGRU(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, latent_dim, heads=4, dropout=0.3):\n",
        "        super(EnhancedHybridGNNVAEGRU, self).__init__()\n",
        "\n",
        "        # Feature attention\n",
        "        self.feature_attention = FeatureAttention(in_channels)\n",
        "\n",
        "        # Multi-layer GNN with residual connections\n",
        "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout)\n",
        "        self.conv2 = GCNConv(hidden_channels * heads, hidden_channels * heads)\n",
        "        self.conv3 = GraphConv(hidden_channels * heads, out_channels)\n",
        "\n",
        "        # Normalization layers\n",
        "        self.norm1 = nn.LayerNorm(hidden_channels * heads)\n",
        "        self.norm2 = nn.LayerNorm(hidden_channels * heads)\n",
        "        self.norm3 = nn.LayerNorm(out_channels)\n",
        "\n",
        "        # VAE components\n",
        "        self.fc_mu = nn.Sequential(\n",
        "            nn.Linear(out_channels, latent_dim * 2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(latent_dim * 2, latent_dim)\n",
        "        )\n",
        "\n",
        "        self.fc_logvar = nn.Sequential(\n",
        "            nn.Linear(out_channels, latent_dim * 2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(latent_dim * 2, latent_dim)\n",
        "        )\n",
        "\n",
        "        # Bidirectional GRU for temporal modeling\n",
        "        self.gru = nn.GRU(\n",
        "            latent_dim,\n",
        "            latent_dim,\n",
        "            num_layers=3,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # MLP for final prediction\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(latent_dim * 2, latent_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(latent_dim, latent_dim // 2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(latent_dim // 2, 1)\n",
        "        )\n",
        "\n",
        "        # Auxiliary classifier\n",
        "        self.aux_classifier = nn.Sequential(\n",
        "            nn.Linear(out_channels, latent_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(latent_dim, 1)\n",
        "        )\n",
        "\n",
        "        # Reconstruction decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, latent_dim * 2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(latent_dim * 2, out_channels),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(out_channels, in_channels)\n",
        "        )\n",
        "\n",
        "    def encode(self, x, edge_index, edge_attr):\n",
        "        # Apply feature attention\n",
        "        x = self.feature_attention(x)\n",
        "\n",
        "        # First graph convolution\n",
        "        x1 = self.norm1(F.leaky_relu(self.conv1(x, edge_index, edge_attr), 0.2))\n",
        "\n",
        "        # Second graph convolution with residual connection\n",
        "        x2 = self.norm2(F.leaky_relu(self.conv2(x1, edge_index), 0.2) + x1)\n",
        "\n",
        "        # Third graph convolution\n",
        "        x3 = self.norm3(F.leaky_relu(self.conv3(x2, edge_index), 0.2))\n",
        "\n",
        "        # VAE encoding\n",
        "        mu = self.fc_mu(x3)\n",
        "        logvar = self.fc_logvar(x3)\n",
        "\n",
        "        return x3, mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        if self.training:\n",
        "            std = torch.exp(0.5 * logvar)\n",
        "            eps = torch.randn_like(std)\n",
        "            return mu + eps * std\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, h=None):\n",
        "        # Encode\n",
        "        x_encoded, mu, logvar = self.encode(x, edge_index, edge_attr)\n",
        "\n",
        "        # Auxiliary prediction directly from encoded features\n",
        "        aux_out = self.aux_classifier(x_encoded)\n",
        "\n",
        "        # Reparameterize\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        # Decode for reconstruction\n",
        "        x_recon = self.decode(z)\n",
        "\n",
        "        # GRU processing\n",
        "        z_sequence = z.unsqueeze(1)  # Add sequence dimension\n",
        "\n",
        "        if h is None:\n",
        "            # Initialize hidden state if not provided\n",
        "            h = torch.zeros(2 * 3, z.size(0), mu.size(1), device=x.device)\n",
        "             # 2 for bidirectional, 3 for num_layers\n",
        "\n",
        "        gru_out, h = self.gru(z_sequence, h)\n",
        "\n",
        "        # Final MLP prediction\n",
        "        main_out = self.mlp(gru_out.squeeze(1))\n",
        "\n",
        "        return main_out, aux_out, x_recon, h, mu, logvar\n"
      ],
      "metadata": {
        "id": "xu5A96SUx8fV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss function\n",
        "\n",
        "def enhanced_loss_function(main_pred, aux_pred, x_recon, x_orig, target, mu, logvar,\n",
        "                           gamma=2.0, alpha=0.25, recon_weight=0.2, kld_weight=0.1, aux_weight=0.3):\n",
        "    # Binary cross entropy with logits for main prediction with focal loss\n",
        "    pt = torch.sigmoid(main_pred.squeeze())\n",
        "    pt = torch.where(target.float() == 1, pt, 1-pt)\n",
        "    focal_weight = alpha * torch.pow(1 - pt, gamma)\n",
        "    # Detach focal_weight to make it non-differentiable\n",
        "    focal_weight = focal_weight.detach()\n",
        "    main_bce = F.binary_cross_entropy_with_logits(\n",
        "        main_pred.squeeze(),\n",
        "        target.float(),\n",
        "        weight=focal_weight,\n",
        "        reduction='mean'\n",
        "    )\n",
        "\n",
        "    # Auxiliary prediction loss\n",
        "    aux_bce = F.binary_cross_entropy_with_logits(\n",
        "        aux_pred.squeeze(),\n",
        "        target.float(),\n",
        "        weight=focal_weight,\n",
        "        reduction='mean'\n",
        "    )\n",
        "\n",
        "    # Reconstruction loss\n",
        "    recon_loss = F.mse_loss(x_recon, x_orig, reduction='mean')\n",
        "\n",
        "    # KL divergence\n",
        "    kld = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    # Total loss\n",
        "    total_loss = main_bce + aux_weight * aux_bce + recon_weight * recon_loss + kld_weight * kld\n",
        "\n",
        "    return total_loss, main_bce, aux_bce, recon_loss, kld\n"
      ],
      "metadata": {
        "id": "CX5gpgLMyGTi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "def load_creditcard_data(path='creditcard.csv'):\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "    df = pd.read_csv(path)\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Keep Time as a feature after normalization\n",
        "    time_scaler = StandardScaler()\n",
        "    df['NormTime'] = time_scaler.fit_transform(df[['Time']])\n",
        "\n",
        "    # Use RobustScaler for Amount to handle outliers better\n",
        "    amount_scaler = RobustScaler()\n",
        "    df['Amount'] = amount_scaler.fit_transform(df[['Amount']])\n",
        "\n",
        "    # Feature engineering: add hour of day as cyclical feature\n",
        "    df['Hour'] = df['Time'] / 3600 % 24\n",
        "    df['Hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 24)\n",
        "    df['Hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 24)\n",
        "\n",
        "    # Add interaction features for V columns\n",
        "    df['V1_V3'] = df['V1'] * df['V3']\n",
        "    df['V4_V12'] = df['V4'] * df['V12']\n",
        "    df['V14_V17'] = df['V14'] * df['V17']\n",
        "\n",
        "    # Drop original Time column\n",
        "    df.drop(columns=['Time', 'Hour'], inplace=True)\n",
        "\n",
        "    # Scale all features\n",
        "    feature_cols = [col for col in df.columns if col != 'Class']\n",
        "    features = df[feature_cols].values\n",
        "\n",
        "    # Standardize all features\n",
        "    scaler = StandardScaler()\n",
        "    features = scaler.fit_transform(features)\n",
        "\n",
        "    labels = df['Class'].values.astype(int)\n",
        "    print(f\"Data loaded: {features.shape[0]} transactions, {features.shape[1]} features\")\n",
        "    print(f\"Fraud cases: {np.sum(labels)} ({np.mean(labels)*100:.4f}%)\")\n",
        "\n",
        "    return features, labels, feature_cols\n"
      ],
      "metadata": {
        "id": "_36Q1BbIyXqw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dynamic graph from features using distance and similarity\n",
        "def create_graph_from_batch(features, labels, k=10, epsilon=0.5):\n",
        "    # Compute cosine similarity\n",
        "    sim = cosine_similarity(features)\n",
        "\n",
        "    # Create edges based on similarity threshold and k-nearest neighbors\n",
        "    edge_index = []\n",
        "    edge_weights = []\n",
        "\n",
        "    for i in range(sim.shape[0]):\n",
        "        # Get indices of top-k similar nodes\n",
        "        neighbors_idx = sim[i].argsort()[-k-1:-1]\n",
        "\n",
        "        for j in neighbors_idx:\n",
        "            # Add edge if similarity is above threshold\n",
        "            if sim[i, j] > epsilon:\n",
        "                edge_index.append([i, j])\n",
        "                edge_weights.append([sim[i, j]])  # Use similarity as edge weight\n",
        "\n",
        "    # If no edges above threshold, fall back to basic k-nn\n",
        "    if len(edge_index) == 0:\n",
        "        for i in range(sim.shape[0]):\n",
        "            neighbors_idx = sim[i].argsort()[-k-1:-1]\n",
        "            for j in neighbors_idx:\n",
        "                edge_index.append([i, j])\n",
        "                edge_weights.append([sim[i, j]])\n",
        "\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    edge_attr = torch.tensor(edge_weights, dtype=torch.float)\n",
        "    x = torch.tensor(features, dtype=torch.float)\n",
        "\n",
        "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr), torch.tensor(labels)\n"
      ],
      "metadata": {
        "id": "LLPlsu1-yhhJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute anomaly scores with ensemble approach\n",
        "def compute_anomaly_scores(main_pred, aux_pred, recon_error, mu, logvar, alpha=0.6, beta=0.2, gamma=0.2):\n",
        "    # Convert predictions to probabilities\n",
        "    main_prob = torch.sigmoid(main_pred).squeeze()\n",
        "    aux_prob = torch.sigmoid(aux_pred).squeeze()\n",
        "\n",
        "    # Compute reconstruction probability (1 - normalized error)\n",
        "    recon_prob = 1 - torch.nn.functional.normalize(recon_error, p=2, dim=0)\n",
        "\n",
        "    # Compute KL divergence\n",
        "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
        "    kl_prob = 1 - torch.nn.functional.normalize(kl_div, p=2, dim=0)\n",
        "\n",
        "    # Ensemble score\n",
        "    ensemble_score = alpha * main_prob + beta * aux_prob + gamma * (recon_prob + kl_prob) / 2\n",
        "\n",
        "    return ensemble_score.detach().cpu().numpy()\n"
      ],
      "metadata": {
        "id": "Nmp_oBJPynaq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced evaluation function\n",
        "def evaluate_model(threshold, main_pred, aux_pred, x_recon, x_orig, mu, logvar, true_labels):\n",
        "    # Compute reconstruction error\n",
        "    recon_error = torch.mean((x_recon - x_orig).pow(2), dim=1)\n",
        "\n",
        "    # Calculate ensemble anomaly scores\n",
        "    anomaly_scores = compute_anomaly_scores(main_pred, aux_pred, recon_error, mu, logvar)\n",
        "\n",
        "    # Binary predictions based on threshold\n",
        "    preds = (anomaly_scores > threshold).astype(int)\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision = precision_score(true_labels, preds, zero_division=0)\n",
        "    recall = recall_score(true_labels, preds, zero_division=0)\n",
        "    f1 = f1_score(true_labels, preds, zero_division=0)\n",
        "    auc = roc_auc_score(true_labels, anomaly_scores)\n",
        "    avg_precision = average_precision_score(true_labels, anomaly_scores)\n",
        "\n",
        "    return precision, recall, f1, auc, avg_precision, anomaly_scores\n"
      ],
      "metadata": {
        "id": "lxBksk4byr_7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced training function with early stopping and learning rate scheduling\n",
        "def train(model, optimizer, features, labels, batch_size=128, epochs=50, patience=10, device='cpu'):\n",
        "    model.train()\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Create scheduler for learning rate reduction\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "    # Setup early stopping\n",
        "    best_loss = float('inf')\n",
        "    no_improve_epochs = 0\n",
        "\n",
        "    # Training statistics\n",
        "    train_losses = []\n",
        "\n",
        "    # Use stratified k-fold for batch creation\n",
        "    skf = StratifiedKFold(n_splits=int(len(labels) / batch_size), shuffle=True, random_state=42)\n",
        "\n",
        "    print(f\"Starting training for {epochs} epochs...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0.0\n",
        "        epoch_main_loss = 0.0\n",
        "        epoch_aux_loss = 0.0\n",
        "        epoch_recon_loss = 0.0\n",
        "        epoch_kld_loss = 0.0\n",
        "        batch_count = 0\n",
        "\n",
        "        for _, batch_idx in skf.split(features, labels):\n",
        "            batch_features = features[batch_idx]\n",
        "            batch_labels = labels[batch_idx]\n",
        "\n",
        "            # Create graph data\n",
        "            graph_data, true_labels = create_graph_from_batch(batch_features, batch_labels)\n",
        "\n",
        "            # Move data to device\n",
        "            graph_data = graph_data.to(device)\n",
        "            true_labels = true_labels.to(device)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            main_out, aux_out, x_recon, _, mu, logvar = model(\n",
        "                graph_data.x, graph_data.edge_index, graph_data.edge_attr\n",
        "            )\n",
        "\n",
        "            # Compute loss\n",
        "            loss, main_loss, aux_loss, recon_loss, kld = enhanced_loss_function(\n",
        "                main_out, aux_out, x_recon, graph_data.x, true_labels, mu, logvar\n",
        "            )\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate losses\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_main_loss += main_loss.item()\n",
        "            epoch_aux_loss += aux_loss.item()\n",
        "            epoch_recon_loss += recon_loss.item()\n",
        "            epoch_kld_loss += kld.item()\n",
        "            batch_count += 1\n",
        "\n",
        "        # Calculate average losses\n",
        "        avg_epoch_loss = epoch_loss / batch_count if batch_count > 0 else 0\n",
        "        avg_main_loss = epoch_main_loss / batch_count if batch_count > 0 else 0\n",
        "        avg_aux_loss = epoch_aux_loss / batch_count if batch_count > 0 else 0\n",
        "        avg_recon_loss = epoch_recon_loss / batch_count if batch_count > 0 else 0\n",
        "        avg_kld_loss = epoch_kld_loss / batch_count if batch_count > 0 else 0\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(avg_epoch_loss)\n",
        "\n",
        "        # Print progress\n",
        "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Avg Loss: {avg_epoch_loss:.4f}, \"\n",
        "                  f\"Main Loss: {avg_main_loss:.4f}, Aux Loss: {avg_aux_loss:.4f}, \"\n",
        "                  f\"Recon Loss: {avg_recon_loss:.4f}, KLD: {avg_kld_loss:.4f}\")\n",
        "\n",
        "        train_losses.append(avg_epoch_loss)\n",
        "\n",
        "        # Check for early stopping\n",
        "        if avg_epoch_loss < best_loss:\n",
        "            best_loss = avg_epoch_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "            no_improve_epochs = 0\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "            if no_improve_epochs >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                break\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
        "\n",
        "    # Load the best model\n",
        "    model.load_state_dict(torch.load('best_model.pt'))\n",
        "    return model, train_losses\n"
      ],
      "metadata": {
        "id": "GIVHoOc5y8yW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, features, labels, batch_size=128, device='cpu'):\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    print(\"\\nEvaluating model...\")\n",
        "\n",
        "    # Setup stratified k-fold for consistent batching\n",
        "    skf = StratifiedKFold(n_splits=max(1, int(len(labels) / batch_size)), shuffle=False)\n",
        "\n",
        "    all_main_outputs = []\n",
        "    all_aux_outputs = []\n",
        "    all_recon_outputs = []\n",
        "    all_originals = []\n",
        "    all_mus = []\n",
        "    all_logvars = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, batch_idx in skf.split(features, labels):\n",
        "            batch_features = features[batch_idx]\n",
        "            batch_labels = labels[batch_idx]\n",
        "\n",
        "            # Create graph data\n",
        "            graph_data, true_labels = create_graph_from_batch(batch_features, batch_labels)\n",
        "\n",
        "            # Move data to device\n",
        "            graph_data = graph_data.to(device)\n",
        "            true_labels = true_labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            main_out, aux_out, x_recon, _, mu, logvar = model(\n",
        "                graph_data.x, graph_data.edge_index, graph_data.edge_attr\n",
        "            )\n",
        "\n",
        "            # Collect all outputs\n",
        "            all_main_outputs.append(main_out)\n",
        "            all_aux_outputs.append(aux_out)\n",
        "            all_recon_outputs.append(x_recon)\n",
        "            all_originals.append(graph_data.x)\n",
        "            all_mus.append(mu)\n",
        "            all_logvars.append(logvar)\n",
        "            all_labels.extend(true_labels.cpu().numpy())\n",
        "\n",
        "    # Concatenate all results\n",
        "    if all_main_outputs:\n",
        "        all_main_outputs = torch.cat(all_main_outputs, dim=0)\n",
        "        all_aux_outputs = torch.cat(all_aux_outputs, dim=0)\n",
        "        all_recon_outputs = torch.cat(all_recon_outputs, dim=0)\n",
        "        all_originals = torch.cat(all_originals, dim=0)\n",
        "        all_mus = torch.cat(all_mus, dim=0)\n",
        "        all_logvars = torch.cat(all_logvars, dim=0)\n",
        "        all_labels = np.array(all_labels)\n",
        "\n",
        "        # Find optimal threshold\n",
        "        anomaly_scores = compute_anomaly_scores(\n",
        "            all_main_outputs, all_aux_outputs,\n",
        "            torch.mean((all_recon_outputs - all_originals).pow(2), dim=1),\n",
        "            all_mus, all_logvars\n",
        "        )\n",
        "\n",
        "        # Try multiple thresholds to find optimal\n",
        "        best_f1 = 0\n",
        "        best_threshold = 0\n",
        "        best_results = None\n",
        "\n",
        "        for percentile in range(90, 99):\n",
        "            threshold = np.percentile(anomaly_scores, percentile)\n",
        "            results = evaluate_model(\n",
        "                threshold, all_main_outputs, all_aux_outputs,\n",
        "                all_recon_outputs, all_originals,\n",
        "                all_mus, all_logvars, all_labels\n",
        "            )\n",
        "\n",
        "            if results[2] > best_f1:  # F1 score is at index 2\n",
        "                best_f1 = results[2]\n",
        "                best_threshold = threshold\n",
        "                best_results = results\n",
        "\n",
        "        # Report results with best threshold\n",
        "        print(f\"\\n--- Final Evaluation Results ---\")\n",
        "        print(f\"Optimal Threshold: {best_threshold:.6f}\")\n",
        "        print(f\"Precision: {best_results[0]:.4f}\")\n",
        "        print(f\"Recall: {best_results[1]:.4f}\")\n",
        "        print(f\"F1-score: {best_results[2]:.4f}\")\n",
        "        print(f\"AUC: {best_results[3]:.4f}\")\n",
        "        print(f\"Average Precision: {best_results[4]:.4f}\")\n"
      ],
      "metadata": {
        "id": "SaM5kgsQzKaf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    set_seed()\n",
        "\n",
        "    # Check for GPU\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load data with improved preprocessing\n",
        "    features, labels, feature_names = load_creditcard_data()\n",
        "\n",
        "    # Split data with stratification\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        features, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "    )\n",
        "\n",
        "    print(f\"Training data: {X_train.shape[0]} samples\")\n",
        "    print(f\"Test data: {X_test.shape[0]} samples\")\n",
        "    print(f\"Fraud cases in training: {np.sum(y_train)} ({np.mean(y_train)*100:.2f}%)\")\n",
        "    print(f\"Fraud cases in test: {np.sum(y_test)} ({np.mean(y_test)*100:.2f}%)\")\n",
        "\n",
        "    # Initialize enhanced model\n",
        "    model = EnhancedHybridGNNVAEGRU(\n",
        "        in_channels=X_train.shape[1],\n",
        "        hidden_channels=64,\n",
        "        out_channels=32,\n",
        "        latent_dim=16,\n",
        "        heads=4,\n",
        "        dropout=0.3\n",
        "    )\n",
        "\n",
        "    # Use Adam optimizer with weight decay for regularization\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "    # Training\n",
        "    print(\"\\n--- Training Started ---\")\n",
        "    model, losses = train(model, optimizer, X_train, y_train, batch_size=128, epochs=50, device=device)\n",
        "\n",
        "    # Evaluation on test set\n",
        "    print(\"\\n--- Evaluating on Test Set ---\")\n",
        "    test_results = evaluate(model, X_test, y_test, batch_size=128, device=device)"
      ],
      "metadata": {
        "id": "N6dVZqQbzLU7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c14b8c9-2371-4e76-8fc7-351912cd26a2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Loading and preprocessing data...\n",
            "Data loaded: 284807 transactions, 35 features\n",
            "Fraud cases: 492 (0.1727%)\n",
            "Training data: 227845 samples\n",
            "Test data: 56962 samples\n",
            "Fraud cases in training: 394 (0.17%)\n",
            "Fraud cases in test: 98 (0.17%)\n",
            "\n",
            "--- Training Started ---\n",
            "Starting training for 50 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Avg Loss: 0.1944, Main Loss: 0.0012, Aux Loss: 0.0007, Recon Loss: 0.9600, KLD: 0.0104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/50, Avg Loss: 0.1821, Main Loss: 0.0004, Aux Loss: 0.0003, Recon Loss: 0.8805, KLD: 0.0550\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50, Avg Loss: 0.1806, Main Loss: 0.0004, Aux Loss: 0.0003, Recon Loss: 0.8724, KLD: 0.0567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/50, Avg Loss: 0.1803, Main Loss: 0.0004, Aux Loss: 0.0003, Recon Loss: 0.8695, KLD: 0.0589\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/50, Avg Loss: 0.1762, Main Loss: 0.0004, Aux Loss: 0.0003, Recon Loss: 0.8444, KLD: 0.0685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/50, Avg Loss: 0.1752, Main Loss: 0.0004, Aux Loss: 0.0003, Recon Loss: 0.8394, KLD: 0.0685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/50, Avg Loss: 0.1744, Main Loss: 0.0004, Aux Loss: 0.0003, Recon Loss: 0.8353, KLD: 0.0691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/50, Avg Loss: 0.1737, Main Loss: 0.0004, Aux Loss: 0.0003, Recon Loss: 0.8314, KLD: 0.0695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/50, Avg Loss: 0.1731, Main Loss: 0.0004, Aux Loss: 0.0003, Recon Loss: 0.8280, KLD: 0.0707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/50, Avg Loss: 0.1729, Main Loss: 0.0004, Aux Loss: 0.0003, Recon Loss: 0.8263, KLD: 0.0711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 394 members, which is less than n_splits=1780.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/50, Avg Loss: 0.1727, Main Loss: 0.0004, Aux Loss: 0.0003, Recon Loss: 0.8251, KLD: 0.0720\n",
            "Training completed in 3589.66 seconds\n",
            "\n",
            "--- Evaluating on Test Set ---\n",
            "\n",
            "Evaluating model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 98 members, which is less than n_splits=445.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Evaluation Results ---\n",
            "Optimal Threshold: 0.312348\n",
            "Precision: 0.0737\n",
            "Recall: 0.8571\n",
            "F1-score: 0.1357\n",
            "AUC: 0.9518\n",
            "Average Precision: 0.6337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BKTk_DeFOV0i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}